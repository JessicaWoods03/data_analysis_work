# -*- coding: utf-8 -*-
"""Iris_Solution.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x2d09fra6rZXYiXr9YEmzAg2Ipo_Igii

<h1><b><font color="green">Visualizing the Data of the Iris Set:</b></font></h1>
Below is the data visualization implemented in <b>Assignment 2 </b>write-up were we used different applications to create histograms and various plots from the Iris dataset. The various calculations include the dot product and calculations of distance between vectors in the data set.
"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
import seaborn as sns

iris = load_iris()
iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
iris_df['species'] = iris.target_names[iris.target]
print(iris_df.head())

# do comparisons

"""<h3><b>Scatterplots:</p></b></h3>

The most common use of the scatter plot is to display the relationship between two variables and observe the nature of such a relationship. The relationships observed can either be positive or negative, non-linear or linear, and/or, strong or weak. We have 4 scatterplots and you can see how strong or weak the relationship is between the points targeted below.
"""

sns.scatterplot(data=iris_df, x='sepal length (cm)', y='sepal width (cm)', hue='species')
plt.show()

sns.scatterplot(data=iris_df, x='petal length (cm)', y='petal width (cm)', hue='species')
plt.show()

from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
# Extract columns
sepal_length = iris_df['sepal length (cm)']
sepal_width = iris_df['sepal width (cm)']
petal_length = iris_df['petal length (cm)']
petal_width = iris_df['petal width (cm)']

ax.scatter( sepal_width, sepal_length, petal_length)
ax.set_xlabel('Sepal Length')
ax.set_ylabel('Sepal Width')
ax.set_zlabel('Petal Length')
ax.set_title('Sepal Length vs. Sepal Width vs. Petal Length')
plt.show()

fig1 = plt.figure()
ax1 = fig1.add_subplot(111, projection='3d')

ax1.scatter( sepal_width, sepal_length, petal_length, c=petal_width, cmap='viridis')
ax1.set_xlabel('Sepal Length')
ax1.set_ylabel('Sepal Width')
ax1.set_zlabel('Petal Length')
ax1.set_title('Sepal Length vs. Sepal Width vs. Petal Length with Petal width')
plt.show()

"""Heat Maps is to better visualize the volume of locations/events within a dataset and assist in directing viewers towards areas on data visualizations that matter most."""

# heat map
sns.heatmap(iris_df[['petal length (cm)','petal width (cm)','sepal length (cm)','sepal width (cm)']], annot=True, cmap='YlGnBu')
plt.show()

"""This is a histogram done with sns, it shows that the data does not have a normal distribution."""

sns.histplot(data=iris_df, x='sepal length (cm)', bins=10)
plt.show()

"""Pairplot visualizes given data to find the relationship between them where the variables can be continuous or categorical."""

sns.pairplot(iris_df)

pip install sweetviz

"""Sweetviz, autoviz,

Sweetviz did all the histograms automatically. Historgrams help us see if the data has normal distribution.
"""

import sweetviz as sv

report = sv.analyze(iris_df)
# Compare the target variable 'species' with other variables


report.show_notebook()
report.show_html()

"""As noted in the write up calculations:
* dot product between the sepal length vector vs the sepal width vector
* the norm of the sepal length vectors
* distance between the sepal length vector and the sepal width vector.

The dot product helps us measu how closely two vectors align. The norm of the vector is the legnth and is referred to as the vector magnitude.
"""

import numpy as np
from scipy import stats

# Calculate the dot product
dot_product = np.dot(sepal_length, sepal_width)

# Print the dot product
print("The dot product: ", dot_product)

# Calculate the norm of the vector
norm = np.linalg.norm(sepal_length)

# Print the norm
print("The norm of the sepal length : ",norm)

# Calculate the distance between the vectors
distance = np.linalg.norm(np.array(sepal_length) - np.array(sepal_width))

# Print the distance
print("The distance: ", distance)

# find the p-value for the HO in sepal length
# Perform Shapiro-Wilk test
statistic, p_value = stats.shapiro(sepal_length)

# Print the test results
print("Test Statistic:", statistic)
print("p-value:", p_value)

"""<p>Since the p-value (0.01018026564270258) is less than the commonly chosen significance level of 0.05 (assuming alpha = 0.05), we have sufficient evidence to reject the null hypothesis. So we will do a Wilcox Rank test. Because as we can see from the historgrams and this test that this is not a normal distribution.</p>

<h1><b><font color="green">Clustering Methods:</b></font></h1>
Below is clustering methods implimented from the <b>Assignment 4 </b>write up. This will implement k-means, PAM, Hierachial Clustering with various calculations to the optimal number of clusters, correlations among dimensions.
"""

# Clustering from Assignment 4 write-up

# Calculate optimal number of clusters:

# Corrilations among dimensions:

#

"""The elbow plot is helps you understand how your data is organised using visual analysis and insight to choosing the optimal value of k."""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

df = iris_df.iloc[:,0:4]
wcss = []


for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=0)
    kmeans.fit(df)
    wcss.append(kmeans.inertia_)
    labels = kmeans.labels_

plt.plot(range(2, 11), wcss, marker='o')
plt.title('Elbow Plot')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')

plt.show()

"""The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion. 'ward' minimizes the variance of the clusters being merged. 'average' uses the average of the distances of each observation of the two sets.<br>
PCA is used for dimensionality reduction / feature selection / representation learning e.g. when the feature space contains too many irrelevant or redundant features.
"""

from sklearn.decomposition import PCA
from sklearn.cluster import AgglomerativeClustering

iris_df = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])

pca = PCA(n_components=2)
pca_features = pca.fit_transform(iris_df)

kmeans = KMeans(n_clusters=2)
kmeans.fit(iris_df)
original_labels = kmeans.labels_

kmeans_pca = KMeans(n_clusters=2)
kmeans_pca.fit(pca_features)
pca_labels = kmeans_pca.labels_


agg_clustering = AgglomerativeClustering(n_clusters=2, linkage='ward')
agg_clustering.fit(iris_df)
original_labels_agg = agg_clustering.labels_

agg_clustering_pca = AgglomerativeClustering(n_clusters=2, linkage='ward')
agg_clustering_pca.fit(pca_features)
pca_labels_agg = agg_clustering_pca.labels_

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

axes[0].scatter(iris_df['sepal length (cm)'], iris_df['sepal width (cm)'], c=original_labels, cmap='viridis')
axes[0].set_title('Original Data Frame Clusters')
axes[0].set_xlabel('Sepal Length (cm)')
axes[0].set_ylabel('Sepal Width (cm)')

axes[1].scatter(pca_features[:, 0], pca_features[:, 1], c=pca_labels, cmap='viridis')
axes[1].set_title('PCA-Transformed Data Frame Clusters')
axes[1].set_xlabel('Principal Component 1')
axes[1].set_ylabel('Principal Component 2')

plt.tight_layout()
plt.show()

""" This creates groups so that objects within a group are similar to each other and different from objects in other groups. Clusters are visually represented in a hierarchical tree called a dendrogram."""

from sklearn.utils.validation import sp
# Heirachial Clustering
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage
print(iris_df.head())

# Perform hierarchical clustering using the 'ward' linkage method
linkage_matrix = linkage(iris_df, method='ward')

# Plot the dendrogram
plt.figure(figsize=(12, 6))
dendrogram(linkage_matrix, labels=iris.target_names[iris.target], leaf_rotation=90)
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Species')
plt.ylabel('Distance')
plt.show()

"""<h1><b><font color="green">HO Testing Below:</font></h1>
This is HO testing from Assignment 2 write up, Assignment 4 write up Hypothesis.

<h4><u><b>Petal Length HO tested:</h4></b></u>
HO: Iris species can be determined by accurate measurements of the sepal length, **petal length**, sepal width.
To reject the HO: Is that the iris species cannot be determined from those measurements.
"""

# Extract the Petal Length data for two groups
group1 = iris_df.loc[iris_df['species'] == 'setosa', 'petal length (cm)']
group2 = iris_df.loc[iris_df['species'] != 'setosa', 'petal length (cm)']

# Perform Mann-Whitney U test
statistic, p_value = stats.mannwhitneyu(group1, group2)

# Print the test results
print("Test Statistic:", statistic)
print("p-value:", p_value)

"""Since the p-value is extremely small (close to 0), we can reject the null hypothesis.

I can test each group to narrow down specifically for all of the petal lengths.
"""

# Extract the Petal Length data for two groups
group1_ver = iris_df.loc[iris_df['species'] == 'versicolor', 'petal length (cm)']
group2_ver = iris_df.loc[iris_df['species'] != 'versicolor', 'petal length (cm)']

# Perform Mann-Whitney U test
statistic_v, p_value_v = stats.mannwhitneyu(group1_ver, group2_ver)

# Print the test results
print("Test Statistic:", statistic_v)
print("p-value:", p_value_v)

"""Since the p-value (0.860606302634834) is greater than the commonly chosen significance level of 0.05 (assuming alpha = 0.05), we fail to reject the null hypothesis. This means that there is insufficient evidence to suggest a significant difference in Petal Length between the 'versicolor' species and the combined data of other species. So best two out of three...lets do the virginica next."""

# Extract the Petal Length data for two groups
group1_vir = iris_df.loc[iris_df['species'] == 'virginica', 'petal length (cm)']
group2_vir = iris_df.loc[iris_df['species'] != 'virginica', 'petal length (cm)']

# Perform Mann-Whitney U test
statistic_vi, p_value_vi = stats.mannwhitneyu(group1_vir, group2_vir)

# Print the test results
print("Test Statistic:", statistic_vi)
print("p-value:", p_value_vi)

"""Viginica also points to the same thing as the first one. The HO with length is to wonder if that can tell you anything about the species. So these tests concluded that the petal length does not tell you anything about the species of the plant. We tested it three times with each species of the plant. Only one test suggested that it did.

<h3><b><u>Sepal Length HO tested:</h3></b></u>
HO: Iris species can be determined by accurate measurements of the <b>sepal length</b>, petal length, sepal width. To reject the HO: Is that the iris species cannot be determined from those measurements.
"""



"""<h3><u><b>Performing HO testing for write-up Assignment 4:</h3></u></b>
HO: Iris dataset is a homogenous collection of species data.<br>
To reject the HO: Is that the iris dataset cannot be considered homogenous and there. ismore than one distinct group that can be analyzed separately.
"""

from scipy.stats import f_oneway
# Check if 'species' column exists in the DataFrame
if 'species' in iris_df.columns:
    # Perform ANOVA test for each feature across different species
    for feature in iris_df.columns:
        if feature != 'species':  # Skip the 'species' column
            grouped_data = [iris_df[feature][iris_df['species'] == species] for species in iris_df['species'].unique()]
            f_statistic, p_value = f_oneway(*grouped_data)
            print(f"Feature: {feature}, p-value: {p_value}")
else:
    print("'species' column not found in DataFrame.")

"""So, if the p-value > 0.05, we would conclude that there is no statistically significant difference or effect in the data, and we would retain the null hypothesis (Ho). Which means the Iris Dataset is a homogenous collection of species. But, the p-value is almost zero, so we reject the null hypothesis. Pvalue = 1.6696691907693826e-31(in one case and other cases are similarly low), the p-value is much smaller than 0.05, indicating strong evidence against the null hypothesis.

<h1><font color="green"><b>Classifications:</b></font></h1>
Exploring Baussuan Methods for the classification portion of the Iris Dataset, along with other classification models to determine which is the best method to solve the iris set. This is assignment 10 of the course work. Our team was hinted to us a ensemble method from Desiscion Trees as well, Naive Bayes Classifier.
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier, HistGradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from mlxtend.plotting import plot_sequential_feature_selection
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, matthews_corrcoef, f1_score
from sklearn.inspection import DecisionBoundaryDisplay
from matplotlib.colors import ListedColormap

# Going to reload, we are quite a ways down from the top load, may get difficult to keep track of
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
import seaborn as sns
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, HistGradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, matthews_corrcoef, f1_score,confusion_matrix, roc_curve, auc
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.model_selection import train_test_split


iris_class = load_iris()
iris_class_df = pd.DataFrame(data=iris_class.data, columns=iris_class.feature_names)
iris_class_df['species'] = iris_class.target_names[iris_class.target]
print(iris_class_df.head())

iris = load_iris()

iris_df = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])
iris_df['target'] = iris.target_names[iris.target]
X = pd.DataFrame(data=iris.data, columns=iris.feature_names)
y = iris.target

classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel="linear"),
    SVC(gamma=2, C=1),
    DecisionTreeClassifier(),
    RandomForestClassifier(),
    MLPClassifier(max_iter=2000),
    AdaBoostClassifier(),
    QuadraticDiscriminantAnalysis(),
    GaussianNB(),
    LinearDiscriminantAnalysis(),
    HistGradientBoostingClassifier(),
]

# My quiet protest started with this block of code that I modified to work
# properly, and Zee deleted my work. Zee did another iris load(), and did more
# library loading, it was a work agianst issue, that I just forked over to continue my
# work.
qda_res = {}

qda = QuadraticDiscriminantAnalysis().fit(X, y)
y_train_pred = qda.predict(X)

qda_res['accuracy'] = accuracy_score(y, y_train_pred)
qda_res['mcc'] = matthews_corrcoef(y, y_train_pred)
qda_res['f1'] = f1_score(y, y_train_pred, average='weighted')
qda_res

# Visualize the results
labels = list(qda_res.keys())
values = list(qda_res.values())

plt.bar(labels, values)
plt.ylabel('Score')
plt.title('Model Performance Metrics')

# Angle the x-axis labels
plt.xticks(rotation=45, ha="right")

plt.tight_layout()
plt.show()

mlp_res = {}
mlp = MLPClassifier(random_state=1, max_iter=2000).fit(X, y)
y_train_pred = mlp.predict(X)

mlp_res['accuracy'] = accuracy_score(y, y_train_pred)
mlp_res['mcc'] = matthews_corrcoef(y, y_train_pred)
mlp_res['f1'] = f1_score(y, y_train_pred, average='weighted')

mlp_res

X2 = X.drop(columns='sepal width (cm)')
knn_res = {}
knn = KNeighborsClassifier(n_neighbors=3).fit(X2,y)
y_train_pred = knn.predict(X2)

knn_res['accuracy'] = accuracy_score(y, y_train_pred)
knn_res['mcc'] = matthews_corrcoef(y, y_train_pred)
knn_res['f1'] = f1_score(y, y_train_pred, average='weighted')

knn_res

svm_res = {}
svm = SVC(kernel="linear").fit(X2,y)
y_train_pred = svm.predict(X2)

svm_res['accuracy'] = accuracy_score(y, y_train_pred)
svm_res['mcc'] = matthews_corrcoef(y, y_train_pred)
svm_res['f1'] = f1_score(y, y_train_pred, average='weighted')

svm_res

svm2_res = {}
svm2 = SVC(gamma=2, C=1).fit(X2,y)
y_train_pred = svm2.predict(X2)

svm2_res['accuracy'] = accuracy_score(y, y_train_pred)
svm2_res['mcc'] = matthews_corrcoef(y, y_train_pred)
svm2_res['f1'] = f1_score(y, y_train_pred, average='weighted')

svm2_res

X3 = X.drop(columns='sepal length (cm)')
dt_res = {}
dt = DecisionTreeClassifier().fit(X3,y)
y_train_pred = dt.predict(X3)

dt_res['accuracy'] = accuracy_score(y, y_train_pred)
dt_res['mcc'] = matthews_corrcoef(y, y_train_pred)
dt_res['f1'] = f1_score(y, y_train_pred, average='weighted')

dt_res

X4 = X2.drop(columns='sepal length (cm)')
rfc_res = {}
rfc = RandomForestClassifier().fit(X4,y)
y_train_pred = rfc.predict(X4)

rfc_res['accuracy'] = accuracy_score(y, y_train_pred)
rfc_res['mcc'] = matthews_corrcoef(y, y_train_pred)
rfc_res['f1'] = f1_score(y, y_train_pred, average='weighted')

rfc_res

ada_res = {}
ada = AdaBoostClassifier().fit(X4,y)
y_train_pred = ada.predict(X4)

ada_res['accuracy'] = accuracy_score(y, y_train_pred)
ada_res['mcc'] = matthews_corrcoef(y, y_train_pred)
ada_res['f1'] = f1_score(y, y_train_pred, average='weighted')

ada_res

hgb_res = {}
hgb = HistGradientBoostingClassifier().fit(X4,y)
y_train_pred = hgb.predict(X4)

hgb_res['accuracy'] = accuracy_score(y, y_train_pred)
hgb_res['mcc'] = matthews_corrcoef(y, y_train_pred)
hgb_res['f1'] = f1_score(y, y_train_pred, average='weighted')

hgb_res

X5 = X4.drop(columns='petal length (cm)')
gnb_res = {}
gnb = GaussianNB().fit(X5,y)
y_train_pred = gnb.predict(X5)

gnb_res['accuracy'] = accuracy_score(y, y_train_pred)
gnb_res['mcc'] = matthews_corrcoef(y, y_train_pred)
gnb_res['f1'] = f1_score(y, y_train_pred, average='weighted')

gnb_res

"""Below is QDA modeling with the Iris Dataset. QDA seems promising but it doesn't really have tuning possibilities. It is all quadratic modeling. So what you see is what you get. QDA could be a good model for the Iris dataset if the classes have different covariance structures, which QDA can capture. However, since the classes in the Iris dataset are relatively well-separated, using QDA might not necessarily lead to significantly better performance."""

# Split the dataset into training and testing sets
from sklearn.metrics import precision_score, recall_score, f1_score
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
# Split the data into  validation set
X_validation, X_test, y_validation, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)
qda = QuadraticDiscriminantAnalysis()

qda.fit(X_train, y_train)

qda_pred = qda.predict(X_test)
# Make predictions on the validation data
y_pred = qda.predict(X_validation)
accuracy = accuracy_score(y_validation, y_pred)
precision = precision_score(y_validation, y_pred, average='weighted')
recall = recall_score(y_validation, y_pred, average='weighted')
f1 = f1_score(y_validation, y_pred, average='weighted')



# Print the calculated metrics
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-Score:", f1)



# Create a confusion matrix
conf_matrix = confusion_matrix(y_test, qda_pred)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, cmap="Blues", fmt="d", xticklabels=iris.target_names, yticklabels=iris.target_names)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix for QDA")
plt.show()

# Obtain the decision function scores for the entire dataset
scores = qda.decision_function(X)

# Add the scores to a DataFrame for plotting
import pandas as pd
scores_df = pd.DataFrame(scores, columns=["QDA Score 1", "QDA Score 2", "QDA Score 3"])
scores_df["Species"] = iris.target_names[y]

# Create a scatter plot matrix
sns.set(style="ticks")
sns.pairplot(scores_df, hue="Species", diag_kind="kde")
plt.show()

# Create a heatmap of the decision function scores
plt.figure(figsize=(10, 8))
sns.heatmap(scores, cmap="YlGnBu", xticklabels=iris.target_names, yticklabels=False)
plt.xlabel("Classes")
plt.ylabel("Samples")
plt.title("Decision Function Scores Heatmap")
plt.show()

"""Naive Bayes classifiers typically work better with discrete data or count data, so using them with the Iris dataset (continuous features) might not provide the best results. Visualizing the Class Probabilities gives you a strong visual of why its not the best option, but it was worth looking at. This tells you how accurate is can classify the iris's."""

# This doesn't have clear descision boundaries like a Rainforest or Descision tree
# Its worth looking at
from sklearn.naive_bayes import MultinomialNB
# Train a Multinomial Naive Bayes classifier
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)

# Make predictions
nb_predictions = nb_model.predict(X_test)
# Get class probabilities is possibly the best way to look at this
nb_probabilities = nb_model.predict_proba(X_test)

plt.figure(figsize=(10, 6))
for i, class_name in enumerate(iris.target_names):
    plt.bar(np.arange(len(X_test)) + i*0.2, nb_probabilities[:, i], width=0.2, label=class_name)

plt.xticks(np.arange(len(X_test)) + len(iris.target_names)*0.2/2, np.arange(len(X_test)))
plt.xlabel('Instance Index')
plt.ylabel('Probability')
plt.title('Class Probabilities - Naive Bayes')
plt.legend()
plt.show()

"""This is a basic decision tree classifier, it shows how predictions are made in the tree."""

from sklearn import tree
import numpy as np
# A modified decision tree was hinted by the professor as a option to solve the
# iris set. So this is a regular decision tree

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

clf = DecisionTreeClassifier()

clf.fit(X_train, y_train)

plt.figure(figsize=(12, 8))
tree.plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)
plt.show()

# Create a Random Forest classifier
rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Fit the classifier to the training data
rf_clf.fit(X_train, y_train)

# Make predictions on the test data
rf_pred = rf_clf.predict(X_test)

# Make predictions on the validation data
y_pred = rf_clf.predict(X_validation)
accuracy = accuracy_score(y_validation, y_pred)
precision = precision_score(y_validation, y_pred, average='weighted')
recall = recall_score(y_validation, y_pred, average='weighted')
f1 = f1_score(y_validation, y_pred, average='weighted')



# Print the calculated metrics
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-Score:", f1)

conf_matrix = confusion_matrix(y_test, rf_pred)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, cmap="Blues", fmt="d", xticklabels=iris.target_names, yticklabels=iris.target_names)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

"""What I found was interesting, is that this gave the best parameters for the Random Forest Classifer in the Iris Data Set."""

# I thought it would be neat to try hyperparameters
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
# Create a Random Forest model
rf = RandomForestClassifier(random_state=42)

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['auto', 'sqrt', 'log2']
}

# Perform Grid Search for hyperparameter tuning
grid_search = GridSearchCV(rf, param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best parameters and model
best_params = grid_search.best_params_
best_rf = grid_search.best_estimator_

# Make predictions using the best model
rf_pred = best_rf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, rf_pred)
print("Best Random Forest Accuracy:", accuracy)
print("Best Parameters:", best_params)

"""The decision scores seem to also favor this model for the Random Forest Classifer to be the best option as all the descisions result in very close terms to 1.0 in the scatter plot and pair plot. Where in the QDA scores didn't look that promising. Even though all the Accuracies are great, I wanted to dive deeper in the scores made on the decisions the models made."""

# Get the decision scores (class probabilities) using predict_proba
decision_scores = rf_clf.predict_proba(X_test)
class_names=iris.target_names
# Visualize decision scores using scatter plots
plt.figure(figsize=(12, 6))

for i, class_name in enumerate(class_names):
    plt.scatter(range(len(y_test)), decision_scores[:, i], label=class_name)

# Visualize decision scores using pair plots
decision_scores_df = pd.DataFrame(decision_scores, columns=class_names)
sns.pairplot(decision_scores_df)
plt.suptitle('Decision Scores (Class Probabilities) Pair Plot', y=1.02)
plt.show()

# Gradient Boosting
from sklearn.ensemble import GradientBoostingClassifier
gb = GradientBoostingClassifier(n_estimators=100, random_state=42)
gb.fit(X_train, y_train)
gb_pred = gb.predict(X_test)
gb_accuracy = accuracy_score(y_test, gb_pred)
print("Gradient Boosting Accuracy:", gb_accuracy)

!pip install xgboost
!pip install lightgbm

"""So the professor asked us to explore ensemble methods with descision trees, this one is similar to Gradient boosting, but has more memory capabilities, think of it as a better calculator, like the calculator from the 70s would be Gradient Boosting, and LightGBM is a 2023 fancy calculator with better memory, more optimizations, even better visial options."""

import xgboost as xgb
from xgboost import plot_tree
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import lightgbm as lgb
from lightgbm import plot_tree

# Create a LightGBM model
lgb_model = lgb.LGBMClassifier(n_estimators=100, random_state=42)
lgb_model.fit(X_train, y_train)
# Split the data into  validation set
X_validation, X_test, y_validation, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)
# Calculate evaluation metrics
# Make predictions on the validation data
y_pred = lgb_model.predict(X_validation)
accuracy = accuracy_score(y_validation, y_pred)
precision = precision_score(y_validation, y_pred, average='weighted')
recall = recall_score(y_validation, y_pred, average='weighted')
f1 = f1_score(y_validation, y_pred, average='weighted')



# Print the calculated metrics
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-Score:", f1)



# Convert the LightGBM model to a Booster
booster = lgb_model.booster_

# Visualize an individual tree
plt.figure(figsize=(20, 10))
plot_tree(booster, tree_index=2, show_info=['split_gain', 'internal_value', 'internal_count', 'leaf_count'])
plt.show()

# The messages below are informational and
# provide details about the training process of the LightGBM model.
# They indicate the number of bins, the dataset's size and features, and the
# starting scores for the training process. These
# messages are normal and are part of the training
# output that helps monitor the progress of the model training.
# You can change the index and run the model again, I got this result when I changed the index
# from 0 to 2
#[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
#[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000045 seconds.
#You can set `force_col_wise=true` to remove the overhead.
#[LightGBM] [Info] Total Bins 86
#[LightGBM] [Info] Number of data points in the train set: 105, number of used features: 4
#[LightGBM] [Info] Start training from score -1.219973
#[LightGBM] [Info] Start training from score -1.043042
#[LightGBM] [Info] Start training from score -1.043042

# Train an XGBoost model
xgb_model = xgb.XGBClassifier()
xgb_model.fit(X_train, y_train)

# Visualize a tree from the model
# Choose a tree to visualize, for example, the first tree (index 0)
plt.figure(figsize=(20, 10))
xgb.plot_tree(xgb_model, num_trees=0, rankdir='LR')
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import StackingClassifier
from lightgbm import LGBMClassifier
# Train base models
# I am going to use the entire data set because the data is too small when it is split for
# this meta model
lgb_model = LGBMClassifier(random_state=42)
rf_model = RandomForestClassifier(random_state=42)

lgb_model.fit(X, y)
rf_model.fit(X, y)

# Generate predictions from base models
lgb_predictions = lgb_model.predict(X_test)
rf_predictions = rf_model.predict(X_test)

# Create meta-model (StackingClassifier) with Logistic Regression as final estimator
meta_model = StackingClassifier(
    estimators=[('lgb', lgb_model), ('rf', rf_model)],
    final_estimator=LogisticRegression(max_iter=1000)
)

# Train the meta-model using base model predictions
meta_model.fit(np.column_stack((lgb_predictions, rf_predictions)), y_test)

# Generate predictions from the stacked model
stacked_predictions = meta_model.predict(np.column_stack((lgb_predictions, rf_predictions)))

# Create confusion matrix for stacked model
confusion = confusion_matrix(y_test, stacked_predictions)

# Visualize the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, cmap="Blues", fmt="d", xticklabels=iris.target_names, yticklabels=iris.target_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - Stacked Model')
plt.show()